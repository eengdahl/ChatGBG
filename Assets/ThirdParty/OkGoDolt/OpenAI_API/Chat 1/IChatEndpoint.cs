using OpenAI_API.Models;
using System;
using System.Collections.Generic;
using System.Threading.Tasks;

namespace OpenAI_API.Chat
{
    /// An interface for ChatEndpoint, the ChatGPT API endpoint. Use this endpoint to send multiple messages and carry on a conversation.
    public interface IChatEndpoint
    {
        /// This allows you to set default parameters for every request, 
        //for example to set a default temperature or max tokens.  
        //For every request, if you do not have a parameter set on the request but do have it set here as a default, 
        //the request will automatically pick up the default value.
        ChatRequest DefaultChatRequestArgs { get; set; }

        /// Creates an ongoing chat which can easily encapsulate the conversation. This is the simplest way to use the Chat endpoint.
        /// defaultChatRequestArgs Allows setting the parameters to use when calling the ChatGPT API. 
        //Can be useful for setting temperature, presence_penalty, and more.  
        ///  returns A Conversation which encapulates a back and forth chat betwen a user and an assistant.
        Conversation CreateConversation(ChatRequest defaultChatRequestArgs = null);


        /// Ask the API to complete the request using the specified parameters. This is non-streaming, so it will wait until the API returns the full result. 
        //Any non-specified parameters will fall back to default values specified in <see cref="DefaultChatRequestArgs"/> if present.
        ///  returns Asynchronously returns the completion result. Look in its ChatResult.Choices property for the results.
        Task<ChatResult> CreateChatCompletionAsync(ChatRequest request);


        /// numOutputs Overrides  ChatRequest.NumChoicesPerMessage as a convenience.
        Task<ChatResult> CreateChatCompletionAsync(ChatRequest request, int numOutputs = 5);



        /// messagesThe array of messages to send to the API
        /// model The model to use. See the ChatGPT models available from ModelsEndpoint.GetModelsAsync()
        /// temperature What sampling temperature to use. Higher values means the model will take more risks.0 - 0.9  
        /// numOutputs How many different choices to request for each prompt.
        /// max_tokens How many tokens to complete to. Can return fewer if a stop sequence is hit.
        /// frequencyPenalty The scale of the penalty for how often a token is used. Should generally be between 0 and 1, 
        /// presencePenalty The scale of the penalty applied if a token is already present at all.  Should generally be between 0 and 1
        /// logitBias Maps tokens to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling.
        /// stopSequences One or more sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.
        /// Asynchronously returns the completion result. Look in its ChatResult.Choices property for the results.
        Task<ChatResult> CreateChatCompletionAsync(IList<ChatMessage> messages, Model model = null, double? temperature = null, double? top_p = null, int? numOutputs = null, int? max_tokens = null, double? frequencyPenalty = null, double? presencePenalty = null, IReadOnlyDictionary<string, float> logitBias = null, params string[] stopSequences);


        /// Ask the API to complete the request using the specified message(s).  
        //Any parameters will fall back to default values specified in DefaultChatRequestArgs if present.
        /// messages The messages to use in the generation.
        /// The  ChatResult with the API response.
        Task<ChatResult> CreateChatCompletionAsync(params ChatMessage[] messages);


        /// Ask the API to complete the request using the specified message(s).  
        /// userMessages The user message or messages to use in the generation. 
        /// returns The ChatResult with the API response.
        Task<ChatResult> CreateChatCompletionAsync(params string[] userMessages);


        /// Ask the API to complete the message(s) using the specified request, and stream the results to the resultHandler as they come in.
        /// request The request to send to the API. This does not fall back to default values specified in DefaultChatRequestArgs
        /// resultHandler An action to be called as each new result arrives, which includes the index of the result in the overall result set.
        Task StreamChatAsync(ChatRequest request, Action<ChatResult> resultHandler);

        /// Ask the API to complete the message(s) using the specified request, and stream the results as they come in.
        /// request The request to send to the API. This does not fall back to default values specified in DefaultChatRequestArgs
        /// returns An async enumerable with each of the results as they come in.
        IAsyncEnumerable<ChatResult> StreamChatEnumerableAsync(ChatRequest request);

        /// messagesThe array of messages to send to the API
        /// model The model to use. 
        /// temperature What sampling temperature to use.
        /// top_p An alternative to sampling with temperature
        /// numOutputs How many different choices to request for each prompt.
        /// max_tokens How many tokens to complete to. Can return fewer if a stop sequence is hit.
        /// frequencyPenalty The scale of the penalty for how often a token is used.  
        /// presencePenalty The scale of the penalty applied if a token is already present at all.
        /// logitBias Maps tokens to an associated bias value from -100 to 100.
        /// stopSequences One or more sequences where the API will stop generating further tokens.
        /// Return An async enumerable with each of the results as they come in.
        IAsyncEnumerable<ChatResult> StreamChatEnumerableAsync(IList<ChatMessage> messages, Model model = null, double? temperature = null, double? top_p = null, int? numOutputs = null, int? max_tokens = null, double? frequencyPenalty = null, double? presencePenalty = null, IReadOnlyDictionary<string, float> logitBias = null, params string[] stopSequences);

        /// Ask the API to complete the message(s) using the specified request, and stream the results to the "resultHandler as they come in.

        /// request The request to send to the API. This does not fall back to default values specified in DefaultChatRequestArgs
        /// resultHandler An action to be called as each new result arrives, which includes the index of the result in the overall result set.
        Task StreamCompletionAsync(ChatRequest request, Action<int, ChatResult> resultHandler);
    }
}